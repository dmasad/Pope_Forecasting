{
 "metadata": {
  "name": "Conclave_Modeling_Tutorial"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How (not) to Forecast the Papal Conclave\n",
      "========================================\n",
      "\n",
      "## David Masad\n",
      "#### david.masad [at] gmail [dot] com / [@badnetworker](https://twitter.com/badnetworker)\n",
      "#### [Department of Computational Social Science](http://css.gmu.edu/), George Mason University"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "**TL;DR:** *This is a simple introduction to a few computational social science topics. It goes over some web scraping, processing the resulting text into a distance matrix, and using it as input into a simple agent-based model.*\n",
      "\n",
      "Forecasting a papal conclave seems like a fun challenge. A relatively small number of cardinal-electors under the age of 80 (115 of them, this time around), get together in the Sistine Chapel and vote four times a day until enough of them agree. Unlike with most elections, there aren't any voter polls, and there's no good historic data: cardinals take a vow of secrecy, so the only thing we know for sure is who is elected at the end of the process. Doctrine holds that the electors are guided by the Holy Spirit, but theology, regional politics, institutional rivalries, and personal relationships undoubtedly play a role as well. \n",
      "\n",
      "So without good statistical data, how can we get our Nate Silver on? One way is an agent-based model (ABM). If we can make a reasonable guess as to how cardinals decides who to vote for we can simulate the conclave, running rounds of virtual votes until a Pope is elected.\n",
      "\n",
      "If we were serious about this, our next step would be to go and talk to Vatican experts to figure out how cardinals might vote. We would try and figure out who shared similar opinions on key issues, how the institutional politics and geographic considerations might play out. We'd look at age, and date of elevation, and we'd do our best to build up a social network -- who has worked with who before? Who are friends and allies, and who are the major rivalries? All of that matters.\n",
      "\n",
      "Remember the title? This is how **not** to forecast the conclave. We aren't going to do all of that. Instead, we'll just wing it. \n",
      "\n",
      "I'm going to propose two simple rules of thumb for how cardinals vote:\n",
      "\n",
      "1. Cardinals will tend to prefer candidates who are similar to themselves.\n",
      "2. As voting progresses, cardinals will vote for a candidate who is similar to themselves and also appears to have a chance of winning.\n",
      "\n",
      "(I'm also going to assume that the cardinals will elect one of their own, since that's been the case since 1378).\n",
      "\n",
      "These rules seem like a good starting point. They're wildly simplistic, but that's not necessarily a bad thing. Zero-intelligence agents have a long(ish) pedigree in ABM, and can be surprisingly successful at generating more complex phenomena. If it looks like we're on the right track, we can try to add additional rules, and see how much they improve the result."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Acquisition, or:\n",
      "### Looking for the car keys underneath the streetlight\n",
      "\n",
      "Now that we have our two rules of thumb, we need a way of figuring out just how similar cardinals are to one another. Again, if we wanted to do this properly, we would consult experts (if we aren't experts ourselves), look at high-quality data, read previous theory, and do all the other things that make up good science. Instead, we're going to do what lazy students have done since the beginning of time (about 2001 or so) -- we're going to consult Wikipedia.\n",
      "\n",
      "This is going to involve two separate steps: first, we need to get the data we want from Wikipedia (web scraping). Next, we need to use it to figure out how similar all our cardinal-electors are to one another."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Wikipedia Scraping\n",
      "\n",
      "This is a seat-of-the-pants project, but it doesn't mean we should just throw *all* good scientific practice out the window. Before we try and predict the next Pope, we should see whether we could have predicted the last one. So let's start by grabbing data on the 2005 conclave."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two important libraries for web scraping with Python: **urllib2**, which is part of the standard library and will read web pages for us, and **[Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/)** which will parse the web page  for us.\n",
      "\n",
      "We'll also use the standard **json** library to store the data we download.\n",
      "\n",
      "Let's start by importing them:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division # Makes division work the way we want it to\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "import json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we define some important web addresses so that we won't need to type them out later. We want the base Wikipedia URL, the URL of Wikipedia's [JSON API](http://www.mediawiki.org/wiki/API:Data_formats) and the actual page with the list of 2005's electors. Finally, we also define an HTML header, which will let Python mimic a web browser when it accesses Wikipedia."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BASE_URL = \"http://en.wikipedia.org\"\n",
      "FIRST_URL = \"http://en.wikipedia.org/wiki/Cardinal_electors_in_Papal_conclave,_2005\"\n",
      "WIKI_API_JSON = \"http://en.wikipedia.org/w/api.php?action=parse&format=json&redirects=true&page=\"\n",
      "\n",
      "header = {'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Web scraping is faster than copying and pasting hundreds of links by hand, but it still requires some work: we need to look at the HTML structure of the target page and find any regularities we can exploit to identify the data we want to extract. In this case, we want to extract the names of the electors, and links to their own Wikipedia pages. \n",
      "\n",
      "I've already done the work of that; look at the source of the page yourself if you want to see it for yourself."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First, we open the page with the data we want:\n",
      "req = urllib2.Request(FIRST_URL,None,header)\n",
      "page = urllib2.urlopen(req)\n",
      "page_text = page.read()\n",
      "\n",
      "# Next, we use Beautiful Soup to find the page elements we care about:\n",
      "soup = BeautifulSoup(page_text)\n",
      "\n",
      "# Get all the cardinal electors from the page\n",
      "ol = soup.findAll(\"ol\")\n",
      "links = []\n",
      "for lst in ol:\n",
      "    items = lst.findAll(\"li\")\n",
      "    for i in items:\n",
      "        links.append(i)\n",
      "\n",
      "# Check how many links we got; should be 115\n",
      "print len(links)\n",
      "\n",
      "# Now extract the name and URL for each elector:\n",
      "# Extract the names and links:\n",
      "electors = []\n",
      "for entry in links:\n",
      "    first_link = entry.find(\"a\")\n",
      "    name = first_link.get_text()\n",
      "    url = first_link.get(\"href\")\n",
      "    electors.append({\"Name\": name, \"url\": url})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "115\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the list of cardinals, we need to get all of their pages. With the list of electors, we wanted only a specific subset of the page; now, we just want the entire page text, so we can use Wikipedia's JSON API."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the text for each via the Wikimedia JSON API:\n",
      "for elector in electors:\n",
      "    url = elector[\"url\"][6:] # Drop the '/wiki/'\n",
      "    page = urllib2.urlopen(WIKI_API_JSON + url).read()\n",
      "    page_json = json.loads(page)\n",
      "    page_text = page_json[\"parse\"][\"text\"].values()[0]\n",
      "    page_soup = BeautifulSoup(page_text)\n",
      "    text = page_soup.get_text()\n",
      "    elector[\"FullText\"] = text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Look through the electors dictionaries -- you'll see that for each cardinal, we now have the full text of their Wikipedia page. \n",
      "\n",
      "We should save this data to keep us from needing to rescrape the data from scratch every time we want to run the model. The easiest way is to simply dump our dictionaries to a JSON file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"ElectorData05.json\", \"wb\") as f:\n",
      "    json.dump(electors, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the text of each cardinal's Wikipedia page, we need to use it to find some similarity measure between them. \n",
      "\n",
      "Specifically: let's the define the similarity between two cardinals as the similarity between their English Wikipedia pages. This is almost certainly a highly flawed measure, but hey, it's easy.\n",
      "\n",
      "More formally, given two Wikipedia pages (or any other text documents), we define the similarity between them as follows:\n",
      "\n",
      "1. Separate each document into words, removing [stop-words](http://en.wikipedia.org/wiki/Stop_words) and punctuation.\n",
      "2. Group the words into n-grams. An n-gram is simply a sequence of n words that come after each other. For example, in *\"The quick brown fox jumped over the lazy dog\"*, unigrams (n=1) are just the words; bigrams (n=2) are (The quick), (quick brown), (brown fox), etc. \n",
      "3. Use these n-grams to create [Term Vectors](http://en.wikipedia.org/wiki/Term_vector_model), which just count the number of times each n-gram appears in the document. \n",
      "4. Finally, compute the [Jaccard Similarity](http://en.wikipedia.org/wiki/Jaccard_index) between the documents. This simply means counting the n-grams that appear in both documents, and dividing that by the total number of n-grams that appear in either document. The higher the fraction of n-grams that appear in both documents, the more similar they are.\n",
      "\n",
      "Let's define some functions that will help us do this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_word(word, punctuation):\n",
      "    \"\"\"\n",
      "    Cleans all the punctuation marks from a word.\n",
      "\n",
      "    Args:\n",
      "        word: target word\n",
      "        punctuation: string of punctuation tokens.\n",
      "    Returns:\n",
      "        The word cleaned of punctuation marks at the beginning and end, and\n",
      "        converted to lower-case.\n",
      "    \"\"\"\n",
      "    try: \n",
      "        while len(word)>0:\n",
      "            while word[0] in punctuation:\n",
      "                word = word[1:]\n",
      "            while word[-1] in punctuation:\n",
      "                word = word[:-1]\n",
      "            break\n",
      "        return word.lower()\n",
      "    \n",
      "    except:\n",
      "        return \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_ngrams(text, n=1, punctuation=\"\"\",\"';:-?!.\"\"\", stop_words=[]):\n",
      "    \"\"\"\n",
      "    Sparse-vectorizer: cleans the text and returns a dictionary of n-gram counts\n",
      "\n",
      "    Args:\n",
      "        text: the input text; assumed to be a sentence.\n",
      "        n: the number of tokens per n-gram. Defaults to 1.\n",
      "        punctuation: String of punctuation tokens to ignore.\n",
      "        stop_words: a list of n-grams to ignore; empty by default.\n",
      "    \n",
      "    Returns:\n",
      "        A term vector, of the form\n",
      "            {term: count, term: count...}\n",
      "    \"\"\"\n",
      "    ngrams = defaultdict(int)\n",
      "\n",
      "    words = [clean_word(word, punctuation) for word in text.split()] # Clean the word list\n",
      "    words = [word for word in words if len(word)>0] #Remove nulls.\n",
      "    while n > 0:\n",
      "        # Repeat for all values >= n:        \n",
      "        for i in range(len(words)-n+1):\n",
      "            new_ngram = []\n",
      "            counter = 0\n",
      "            while len(new_ngram)<n:\n",
      "                new_ngram.append(words[i+counter])\n",
      "                counter += 1\n",
      "            new_ngram = tuple(new_ngram)\n",
      "            if new_ngram not in stop_words:\n",
      "                ngrams[new_ngram] += 1\n",
      "        n -= 1\n",
      "    \n",
      "    return ngrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jaccard_similarity(vector1, vector2):\n",
      "    '''\n",
      "    Compute Jaccard Similarity between two sparse vectors,\n",
      "    represented as dicts.\n",
      "    '''\n",
      "\n",
      "    all_keys = set(vector1.keys() + vector2.keys())\n",
      "    union = len(all_keys)\n",
      "    intersection = 0\n",
      "    for key in all_keys:\n",
      "        if key in vector1 and key in vector2:\n",
      "            intersection += 1\n",
      "    return intersection / union"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_distance_matrix(sparse_vectors, metric, inverse=False):\n",
      "    '''\n",
      "    Inputs are sparse vectors and a metric function; \n",
      "    pre-computes all the distances and returns a function \n",
      "    for accessing them. \n",
      "\n",
      "    Args:\n",
      "        sparse_vectors: dictionary of {'name': {sparse: vectors},...}\n",
      "        metric: distance metric function\n",
      "        inverse: bool on whether to invert the metric.\n",
      "                If False, smaller is more similar.\n",
      "\n",
      "    Returns:\n",
      "        A closure-defined function wrapped around a pre-computed Similarity\n",
      "        matrix.\n",
      "        \n",
      "    '''\n",
      "    distances = {}\n",
      "    for doc1, vector1 in sparse_vectors.items():\n",
      "        distances[doc1] = {}\n",
      "        for doc2, vector2 in sparse_vectors.items():\n",
      "            if doc1 != doc2 and doc2 not in distances:\n",
      "                distances[doc1][doc2] = metric(vector1, vector2)\n",
      "                if inverse: \n",
      "                    distances[doc1][doc2] = 1 - distances[doc1][doc2]\n",
      "\n",
      "    def find_distance(doc1, doc2):\n",
      "        '''\n",
      "        Find the distance between doc1 and doc2\n",
      "        '''\n",
      "        if doc2 not in distances[doc1]:\n",
      "            doc1, doc2 = doc2, doc1\n",
      "        return distances[doc1][doc2]\n",
      "    return find_distance"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great! Now, how do we use our newly-defined functions? Let's actually apply them to our data. If you have the excellent [NLTK](http://nltk.org/) library installed, you can use its list of stopwords; otherwise, I'm copying it here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define stop-words:\n",
      "try:\n",
      "    from nltk.corpus import stopwords\n",
      "    all_stopwords = stopwords.words(\"english\") + [\"[edit]\"] \n",
      "except:\n",
      "    all_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
      "        'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
      "         'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', \n",
      "         'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
      "         'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', \n",
      "         'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
      "         'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',\n",
      "         'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', \n",
      "         'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n",
      "         'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
      "         'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', \n",
      "         'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
      "         'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', '[edit]']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate the term vector for each page:\n",
      "for elector in electors:\n",
      "    elector[\"TextVector\"] = find_ngrams(elector[\"FullText\"], n=3, stop_words = all_stopwords)\n",
      "\n",
      "vectors = {} # Dictionary to hold our vectors\n",
      "# Generate the distance matrix, and the function that accesses it:\n",
      "for elector in electors:\n",
      "    vectors[elector[\"Name\"]] = elector[\"TextVector\"]\n",
      "find_distance = create_distance_matrix(vectors, jaccard_similarity, inverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see how we're doing, let's generate a simple data quality test and see which cardinal is most similar (by this measure) to the rest of the cardinals."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean_similarities = {}\n",
      "# Iterate over each elector, and compute his similarity to each other one.\n",
      "for elector1 in electors:\n",
      "    total_similarities = 0\n",
      "    for elector2 in electors:\n",
      "        if elector1 == elector2: continue # Don't compare someone to themselves\n",
      "        similarity = find_distance(elector1[\"Name\"], elector2[\"Name\"])\n",
      "        total_similarities += similarity\n",
      "    mean_similarity = total_similarities / (len(electors)-1)\n",
      "    mean_similarities[elector1[\"Name\"]] = mean_similarity\n",
      "\n",
      "# Print the electors, sorted from highest to lowest mean similarity:\n",
      "print sorted(mean_similarities.keys(), key=lambda x: mean_similarities[x], reverse=True)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Joseph Ratzinger', u'George Pell', u'Jean-Marie Lustiger', u\"Keith O'Brien\", u'Carlo Maria Martini', u'Francis George', u'Tarcisio Bertone', u'Angelo Scola', u'Roger Mahony', u\"Cormac Murphy-O'Connor\", u'Edward Egan', u'Miguel Obando y Bravo', u'Bernard Francis Law', u'Peter Turkson', u'Justin Rigali', u'Angelo Sodano', u'J\\xf3zef Glemp', u'Christoph Sch\\xf6nborn', u'Dar\\xedo Castrill\\xf3n Hoyos', u'Marc Ouellet', u'Dionigi Tettamanzi', u'Desmond Connell', u'Francis Arinze', u'Lubomyr Husar', u'Adam Maida', u'Walter Kasper', u'Fr\\xe9d\\xe9ric Etsou-Nzabi-Bamungwabi', u'Renato Raffaele Martino', u'Miloslav Vlk', u'Jean-Claude Turcotte', u'Thomas Stafford Williams', u'Godfried Danneels', u'Giovanni Battista Re', u'Theodore McCarrick', u'Jos\\xe9 da Cruz Policarpo', u'Jorge Bergoglio', u'Eus\\xe9bio Scheid', u'James Francis Stafford', u'Mar Varkey Vithayathil', u'Juan Luis Cipriani Thorne', u'Ricardo Vidal', u'Camillo Ruini', u'Ivan Dias', u'Alfonso L\\xf3pez Trujillo', u'Michele Giordano', u'Joachim Meisner', u'Jean-Louis Tauran', u'Edmund Casimir Szoka', u'\\xd3scar Andr\\xe9s Rodr\\xedguez Maradiaga', u'Jaime Lucas Ortega y Alamino', u'Juli\\xe1n Herranz Casado', u'Giacomo Biffi', u'William Keeler', u'Ignace Daoud', u'William Wakefield Baum', u'Karl Lehmann', u'Norberto Rivera Carrera', u'Jorge Arturo Medina Est\\xe9vez', u'Jos\\xe9 Freire Falc\\xe3o', u'Adrianus Johannes Simonis', u'Francisco Javier Err\\xe1zuriz Ossa', u'Javier Lozano Barrag\\xe1n', u'L\\xe1szl\\xf3 Paskai', u'Michael Kitbunchu', u'Georg Sterzinsky', u'Crescenzio Sepe', u'Zenon Grocholewski', u'Aloysius Ambrozic', u'Juan Sandoval \\xcd\\xf1iguez', u'Francesco Marchisano', u'Mario Francesco Pompedda', u'Cl\\xe1udio Hummes', u'Vinko Pulji\\u0107', u'Emmanuel Wamala', u'J\\u0101nis Puj\\u0101ts', u'Julius Riyadi Darmaatmadja', u'Sergio Sebastiani', u'Ennio Antonelli', u'Armand Razafindratandra', u'Attilio Nicora', u'Agostino Cacciavillan', u'Jean-Baptiste Ph\\u1ea1m Minh M\\u1eabn', u'Telesphore Toppo', u'Polycarp Pengo', u'P\\xe9ter Erd\\u0151', u'Antonio Mar\\xeda Rouco Varela', u'Wilfrid Fox Napier', u'Stephen Fumio Hamao', u'Eduardo Mart\\xednez Somalo', u'Paul Poupard', u'Pedro Rubiano S\\xe1enz', u'Carlos Amigo Vallejo', u'Rodolfo Quezada Toru\\xf1o', u'Friedrich Wetter', u'Anthony Olubumni Okogie', u'Francisco \\xc1lvarez Mart\\xednez', u'Nicol\\xe1s de Jes\\xfas L\\xf3pez Rodr\\xedguez', u'Henri Schwery', u'Marian Jaworski', u'Geraldo Majella Agnelo', u'Peter Shirayanagi', u'Franciszek Macharski', u'Salvatore De Giorgi', u'Christian Tumi', u'Julio Terrazas Sandoval', u'Philippe Barbarin', u'Josip Bozani\\u0107', u'Audrys Juozas Ba\\u010dkis', u'Ricardo Mar\\xeda Carles Gord\\xf3', u'Bernard Agr\\xe9', u'Bernard Panafieu', u'Gabriel Zubeir Wako', u'Marco C\\xe9', u'Jos\\xe9 Saraiva Martins', u'Severino Poletto']\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Look at the first name on the list. Well, what do you know?\n",
      "\n",
      "Of course, you *do* know. Since Ratzinger was elected Pope, his Wikipedia page is probably longer and more detailed than anyone else's, so of course there will be more similarities. [According to Wikipedia](http://en.wikipedia.org/wiki/Papal_conclave,_2005#Results_of_the_first_ballot), Carlo Maria Martini (ranked 5th on the list above) and Camillo Ruini (ranked far lower) were the other two serious contenders, while Jean-Marie Lustiger (ranked 3rd) doesn't even appear on the [list of potential candidates](http://en.wikipedia.org/wiki/List_of_papabili_in_the_2005_papal_conclave). So, far from perfect. But not terrible either."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Zero-Intelligence Cardinals\n",
      "\n",
      "Remember the two rules I proposed before:\n",
      "\n",
      "1. Cardinals will tend to prefer candidates who are similar to themselves.\n",
      "2. As voting progresses, cardinals will vote for a candidate who is similar to themselves and also appears to have a chance of winning.\n",
      "\n",
      "Now we need to implement them as an agent-based model. We'll do this by first creating two classes: one for the Elector agent, and another to manage the overall election process. Each elector needs to be able to implement both of these rules.\n",
      "\n",
      "This isn't quite a zero-intelligence agent, but it's close. We're giving our agents only the most minimal rules and decisionmaking abilities, and we'll see how they perform."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "It's important to include some randomness in our model, to account for all the things\n",
      "we don't know or understand. A completely deterministic model will be brittle -- if our\n",
      "assumptions or data aren't perfect, the model will produce only wrong results. Randomness will let us\n",
      "explore and test varied processes and outcomes.\n",
      "'''\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "class Elector(object):\n",
      "    '''\n",
      "    Elector agent.\n",
      "\n",
      "    Attributes:\n",
      "        opinions: A dictionary of options to vote on, and associated weights:\n",
      "            {Outcome1: Preference, Outcome2: Preference...}\n",
      "    \n",
      "    Methods:\n",
      "        __init__(opinion): Instantiate the agent with an opinion dictionary\n",
      "        first_pick(): Get the agent's initial vote,.\n",
      "        next_vote(previous_votes): Get an agent's vote after the results of the\n",
      "            previous round of voting are already known.\n",
      "\n",
      "    '''\n",
      "\n",
      "    def _weighted_random(self, weight_dict):\n",
      "        '''\n",
      "        Chooses a possibility based on their associated weights.\n",
      "\n",
      "        Args:\n",
      "            weight_dict: A dictionary of the form\n",
      "                    {Outcome1: weight1, Outcome2: weight2}\n",
      "                Note that weights must not sum to 1; this method\n",
      "                scales them automatically.\n",
      "        Returns:\n",
      "            One of the outcomes specified in weight_dict\n",
      "        '''\n",
      "        total = sum(weight_dict.values())\n",
      "        choice = random.random() * total\n",
      "        counter = 0\n",
      "        for outcome, weight in weight_dict.items():\n",
      "            if choice < counter + weight:\n",
      "                return outcome\n",
      "            else:\n",
      "                counter += weight\n",
      "\n",
      "    def __init__(self, opinions=None):\n",
      "        '''\n",
      "        Initialize an Elector agent.\n",
      "\n",
      "        Args:\n",
      "            opinions: A dictionary of the form\n",
      "                {option1: preference, option2: prefernce}\n",
      "        '''\n",
      "\n",
      "        self.opinions = opinions\n",
      "\n",
      "    def first_pick(self):\n",
      "        '''\n",
      "        Choose an initial vote.\n",
      "        Return:\n",
      "            An option chosen randomly in proportion to the opinion.\n",
      "        '''\n",
      "        return self._weighted_random(self.opinions)\n",
      "\n",
      "    def next_vote(self, previous_votes):\n",
      "        '''\n",
      "        Decide the next vote given the outcome of the previous vote.\n",
      "        Args:\n",
      "            previous_votes: A dictionary associating outcomes with votes:\n",
      "                {option1: votes for option1, option2: votes for option 2...}\n",
      "\n",
      "        Returns:\n",
      "            The option that maximizes votes * preference\n",
      "        '''\n",
      "        max_weight = 0\n",
      "        max_outcome = None\n",
      "\n",
      "        for option, pref in self.opinions.items():\n",
      "            wgt = pref * previous_votes[option]\n",
      "            if wgt > max_weight:\n",
      "                max_outcome = option\n",
      "                max_weight = wgt\n",
      "\n",
      "        return max_outcome"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we need to create a way to manage the election process. There are different ways to do this, but a typical way is through another class; there will be only one instance of it per run of the model, and it will store the elector agents and the outcome of each vote, and also the parameters of the model, such as the fraction required to win."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Election(object):\n",
      "    '''\n",
      "    Supervisor class for elections.\n",
      "\n",
      "    Attributes:\n",
      "        electors: Dictionary of elector agents\n",
      "        options: Options to be voted on\n",
      "        elector_count: How many electors there are\n",
      "        max_rounds: Maximum rounds for voting to go on for\n",
      "        fraction_required: What fraction of votes needed to win\n",
      "        rounds: How many rounds of voting have there been\n",
      "        vote_rounds: A list of dictionaries, one for each round of voting\n",
      "            with options as the keys and votes received as the values.\n",
      "        winner: Who has finally won the election\n",
      "\n",
      "    Methods:\n",
      "        __init__(options, fraction_required, max_rounds): Create a new Election\n",
      "        add_elector(preference_set, name): Add a new elector with a specified\n",
      "            dictionary of preferences\n",
      "        vote(): Execute the next round of voting.\n",
      "        run_elections(): Run an entire election process until a winner is chosen\n",
      "    '''\n",
      "\n",
      "    def __init__(self, options, fraction_required=0.5, max_rounds=None):\n",
      "        '''\n",
      "        Initiate a new election model.\n",
      "        Args:\n",
      "            options: List of possible outcomes to be voted on\n",
      "            fraction_required: Percent of votes needed to win\n",
      "            max_rounds: Maximum number of rounds of voting.\n",
      "        '''\n",
      "        self.winner = None\n",
      "        self.options = options\n",
      "        self.electors = {}\n",
      "        self.rounds = 0\n",
      "        self.elector_count = 0\n",
      "        self.fraction_required = fraction_required\n",
      "        self.max_rounds = max_rounds\n",
      "        self.vote_rounds = []\n",
      "\n",
      "    def add_elector(self, preference_set, name=None):\n",
      "        '''\n",
      "        Add a new elector with the given prefernce set.\n",
      "        Args:\n",
      "            preference_set: A preference set dictionary\n",
      "            name: Name for the agent, if not just a number.\n",
      "        '''\n",
      "        if name is None:\n",
      "            name = self.elector_count\n",
      "\n",
      "        new_elector = Elector(preference_set)\n",
      "        self.electors[name] = new_elector\n",
      "        self.elector_count += 1\n",
      "\n",
      "    def vote(self):\n",
      "        '''\n",
      "        A single round of voting.\n",
      "        '''\n",
      "        votes = {}\n",
      "        for option in self.options:\n",
      "            votes[option] = 0\n",
      "\n",
      "        for elector in self.electors.values():\n",
      "            if self.rounds == 0:\n",
      "                vote = elector.first_pick()\n",
      "            else:\n",
      "                vote = elector.next_vote(self.vote_rounds[-1])\n",
      "            votes[vote] += 1\n",
      "\n",
      "        self.vote_rounds.append(votes)\n",
      "        self.rounds += 1\n",
      "        return votes\n",
      "\n",
      "    def run_elections(self):\n",
      "        '''\n",
      "        Run an entire election.\n",
      "\n",
      "        Iterates the vote() method until one outcome gets fraction_required of\n",
      "        the votes, or maximum rounds reached. Populates the winner attribute.\n",
      "\n",
      "        TODO: This could be more robust, and include rules to handle ties, \n",
      "        or circumstances where fraction_required is less than 50%.\n",
      "\n",
      "        '''\n",
      "\n",
      "        while self.winner is None and (self.rounds < self.max_rounds or \n",
      "                                            self.max_rounds is None): \n",
      "            election_results = self.vote()\n",
      "            for result, votes in election_results.items():\n",
      "                # Note: Doesn't hande fraction_required < 0.5 well yet:\n",
      "                if votes/self.elector_count >= self.fraction_required:\n",
      "                    self.winner = result\n",
      "\n",
      "        # Maximum rounds reached, pick the outcome with a plurality:\n",
      "        if self.winner is None and self.rounds >= self.max_rounds:\n",
      "            # Sort by votes, descending:\n",
      "            sorted_votes = sorted(election_results.keys(), \n",
      "                key=lambda x: election_results[x], reverse=True)\n",
      "            # Get the top vote-getter\n",
      "            self.winner = sorted_votes[0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We should test our code with some quick made-up data to make sure it's doing what we want it to:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Some aritrary voter preferences:\n",
      "elector_prefs = [\n",
      "                 {\"A\": 1, \"B\": 2, \"C\": 3},\n",
      "                 {\"A\": 1, \"B\": 3, \"C\": 2},\n",
      "                 {\"A\": 3, \"B\": 2, \"C\": 1},\n",
      "                 {\"A\": 3, \"B\": 2, \"C\": 1}]\n",
      "\n",
      "# Creating the election supervisor and adding the electors:\n",
      "election = Election(['A', 'B', 'C'], max_rounds = 1000)\n",
      "for pref_set in elector_prefs:\n",
      "    election.add_elector(pref_set)\n",
      "# Run voting and look at the outcome:\n",
      "election.run_elections()\n",
      "print \"The winner is:\", election.winner\n",
      "print \"Rounds: \", election.rounds\n",
      "print \"Voting outcomes: \", election.vote_rounds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The winner is: B\n",
        "Rounds:  1\n",
        "Voting outcomes:  [{'A': 2, 'C': 0, 'B': 2}]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# For further validation, let's run 1000 rounds and see the results:\n",
      "results = {\"A\": 0, \"B\": 0, \"C\": 0}\n",
      "for i in range(1000):\n",
      "    election = Election(['A', 'B', 'C'], max_rounds=1000)\n",
      "    for pref_set in elector_prefs:\n",
      "        election.add_elector(pref_set)\n",
      "    election.run_elections()\n",
      "    results[election.winner] += 1\n",
      "print \"Count of voting results:\"\n",
      "print results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Count of voting results:\n",
        "{'A': 282, 'C': 279, 'B': 439}\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Okay, it seems to work! Notice the proportions of the results: even though 'A' seems to only have slightly more support than 'C', it's enough to skew the outcome substantially. This may be a problem (or may end up being positive), but for now let's press on.\n",
      "\n",
      "Now that we have our model, we need to populate it with the data we collected before. Recall that we defined the **find_distance** function to find the similarity between two cardinals. Now we use it to find each elector's preference set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "elector_prefs = {}\n",
      "for elector in electors:\n",
      "    prefs = {}\n",
      "    for cardinal in electors:\n",
      "        if elector == cardinal: continue # Assume cardinals don't vote for themselves\n",
      "        prefs[cardinal[\"Name\"]] = find_distance(elector[\"Name\"], cardinal[\"Name\"])\n",
      "    elector_prefs[elector[\"Name\"]] = prefs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we define an election object to run the elections, and populate it with the prefences we just found. The elections require a two-thirds majority, and there's no limit to the number of rounds -- the longest-ever papal election lasted from 1268 to 1271.\n",
      "\n",
      "Once the conclave is defined, let's run it and see what happens:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conclave = Election(elector_prefs.keys(), fraction_required=0.66)\n",
      "for name, preferences in elector_prefs.items():\n",
      "    conclave.add_elector(preferences, name)\n",
      "\n",
      "conclave.run_elections()\n",
      "print \"The winner is\", conclave.winner, \"in\", conclave.rounds, \"rounds.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The winner is Sergio Sebastiani in 2 rounds.\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unless you were really lucky, the result probably wasn't Ratzinger. But each run of the model will have a different outcome. To get more robust results, we should run it many times and count the outcomes. Like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outcomes = {}\n",
      "rounds = []\n",
      "for i in range(1000):\n",
      "    conclave = Election(vectors.keys(), 0.66, 10000)\n",
      "    for name, preferences in elector_prefs.items():\n",
      "        conclave.add_elector(preferences, name)\n",
      "    conclave.run_elections()\n",
      "    \n",
      "    rounds.append(conclave.rounds)\n",
      "    if conclave.winner in outcomes:\n",
      "        outcomes[conclave.winner] += 1\n",
      "    else:\n",
      "        outcomes[conclave.winner] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we just sort the list of outcomes and see who won how many times:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ranked_outcomes = sorted(outcomes.keys(), key=lambda x: outcomes[x], reverse=True)\n",
      "print \"Top 20 candidates:\"\n",
      "for outcome in ranked_outcomes[:20]:\n",
      "    print outcome, outcomes[outcome]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 20 candidates:\n",
        "Joseph Ratzinger 32\n",
        "Keith O'Brien 24\n",
        "Bernard Francis Law 23\n",
        "George Pell 23\n",
        "Edward Egan 20\n",
        "Cormac Murphy-O'Connor 19\n",
        "Jean-Marie Lustiger 19\n",
        "Tarcisio Bertone 18\n",
        "Marc Ouellet 18\n",
        "Carlo Maria Martini 18\n",
        "Miguel Obando y Bravo 18\n",
        "Francis George 16\n",
        "Angelo Sodano 15\n",
        "Roger Mahony 15\n",
        "Dionigi Tettamanzi 15\n",
        "Jean-Claude Turcotte 15\n",
        "J\u00f3zef Glemp 14\n",
        "Peter Turkson 14\n",
        "Ignace Daoud 14\n",
        "Fr\u00e9d\u00e9ric Etsou-Nzabi-Bamungwabi 13\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hopefully Ratzinger comes out on top this time, or at least close to it. As you can see, running the model multiple times will give us more robust results; it will also give us an idea of candidates' relative chances.\n",
      "\n",
      "This also helps highlight some bias in the model. Notice that the English-speaking cardinals are coming up close to the top, higher than their chances probably actually are. This is probably because their English Wikipedia pages will be longer and more detailed than those of cardinals from other parts of the world. \n",
      "\n",
      "Nevertheless, this doesn't seem *terrible*. It's possibly even better than random. So, we press on!\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##*Back to the Future!*\n",
      "\n",
      "Now that we've tested the model against historic data, it's time to try and use it for forecasting. We just need to repeat all the steps above: acquire data (this time for the 2013 electors), and feed it to the election model that we've already created."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BASE_URL = \"http://en.wikipedia.org\"\n",
      "FIRST_URL = \"http://en.wikipedia.org/wiki/Cardinal_electors_for_the_papal_conclave,_2013\"\n",
      "WIKI_API_JSON = \"http://en.wikipedia.org/w/api.php?action=parse&format=json&redirects=true&page=\"\n",
      "header = {'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[The Wikipedia page for the 2013 electors](http://en.wikipedia.org/wiki/Cardinal_electors_for_the_papal_conclave,_2013) is formatted a bit differently from the one we looked at before, so we need to tweak our scraping code accordingly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First, we open the page with the data we want:\n",
      "req = urllib2.Request(FIRST_URL,None,header)\n",
      "page = urllib2.urlopen(req)\n",
      "page_text = page.read()\n",
      "\n",
      "# Next, we use Beautiful Soup to find the page elements we care about:\n",
      "soup = BeautifulSoup(page_text)\n",
      "\n",
      "# Get all the cardinal electors from the page\n",
      "links = []\n",
      "table = soup.find(\"table\", \"wikitable sortable\")\n",
      "rows = table.findAll(\"tr\")\n",
      "for row in rows:\n",
      "    link = row.find(\"span\", \"vcard\")\n",
      "    links.append(link)\n",
      "\n",
      "# Check how many links we got; should be 115\n",
      "print len(links)\n",
      "\n",
      "# Now extract the name and URL for each elector:\n",
      "# Extract the names and links:\n",
      "electors = []\n",
      "for entry in links:\n",
      "    if entry is None: continue # Skip in case of bad entry\n",
      "    first_link = entry.find(\"a\")\n",
      "    name = first_link.get_text()\n",
      "    url = first_link.get(\"href\")\n",
      "    electors.append({\"Name\": name, \"url\": url})\n",
      "\n",
      "# Get the text for each via the Wikimedia JSON API:\n",
      "for elector in electors:\n",
      "    url = elector[\"url\"][6:] # Drop the '/wiki/'\n",
      "    page = urllib2.urlopen(WIKI_API_JSON + url).read()\n",
      "    page_json = json.loads(page)\n",
      "    page_text = page_json[\"parse\"][\"text\"].values()[0]\n",
      "    page_soup = BeautifulSoup(page_text)\n",
      "    text = page_soup.get_text()\n",
      "    elector[\"FullText\"] = text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "116\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Save:\n",
      "with open(\"ElectorData13.json\", \"wb\") as f:\n",
      "    json.dump(electors, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the raw data, we just build up the similarity matrix, and input it into the model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate the term vector for each page:\n",
      "for elector in electors:\n",
      "    elector[\"TextVector\"] = find_ngrams(elector[\"FullText\"], n=3, stop_words = all_stopwords)\n",
      "\n",
      "vectors = {} # Dictionary to hold our vectors\n",
      "# Generate the distance matrix, and the function that accesses it:\n",
      "for elector in electors:\n",
      "    vectors[elector[\"Name\"]] = elector[\"TextVector\"]\n",
      "find_distance = create_distance_matrix(vectors, jaccard_similarity, inverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Finding similarities:\n",
      "elector_prefs = {}\n",
      "for elector in electors:\n",
      "    prefs = {}\n",
      "    for cardinal in electors:\n",
      "        if elector == cardinal: continue # Assume cardinals don't vote for themselves\n",
      "        prefs[cardinal[\"Name\"]] = find_distance(elector[\"Name\"], cardinal[\"Name\"])\n",
      "    elector_prefs[elector[\"Name\"]] = prefs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run once:\n",
      "conclave = Election(elector_prefs.keys(), fraction_required=0.66)\n",
      "for name, preferences in elector_prefs.items():\n",
      "    conclave.add_elector(preferences, name)\n",
      "\n",
      "conclave.run_elections()\n",
      "print \"The winner is\", conclave.winner, \"in\", conclave.rounds, \"rounds.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The winner is Jos\u00e9 Policarpo in 2 rounds.\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run 1,000  iterations and tally up the results:\n",
      "outcomes = {}\n",
      "rounds = []\n",
      "for i in range(1000):\n",
      "    conclave = Election(vectors.keys(), 0.66, 10000)\n",
      "    for name, preferences in elector_prefs.items():\n",
      "        conclave.add_elector(preferences, name)\n",
      "    conclave.run_elections()\n",
      "    \n",
      "    rounds.append(conclave.rounds)\n",
      "    if conclave.winner in outcomes:\n",
      "        outcomes[conclave.winner] += 1\n",
      "    else:\n",
      "        outcomes[conclave.winner] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ranked_outcomes = sorted(outcomes.keys(), key=lambda x: outcomes[x], reverse=True)\n",
      "print \"Ranked candidates:\"\n",
      "for outcome in ranked_outcomes:\n",
      "    print outcome, outcomes[outcome]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ranked candidates:\n",
        "Donald Wuerl 28\n",
        "Raymond Leo Burke 25\n",
        "George Pell 25\n",
        "Luis Antonio Tagle 23\n",
        "Edwin Frederick O'Brien 22\n",
        "Timothy M. Dolan 22\n",
        "Christoph Sch\u00f6nborn 18\n",
        "Roger Mahony 17\n",
        "Dionigi Tettamanzi 16\n",
        "George Alencherry 16\n",
        "William Levada 15\n",
        "Peter Turkson 15\n",
        "Stanislaw Dziwisz 15\n",
        "Ra\u00fal Eduardo Vela Chiriboga 14\n",
        "Francis George 14\n",
        "Marc Ouellet 14\n",
        "Se\u00e1n Brady 14\n",
        "Walter Kasper 14\n",
        "Se\u00e1n Patrick O'Malley 14\n",
        "Angelo Scola 14\n",
        "Baselios Cleemis 13\n",
        "Giovanni Battista Re 13\n",
        "Jos\u00e9 Policarpo 13\n",
        "Thomas Christopher Collins 13\n",
        "Odilo Scherer 12\n",
        "John Onaiyekan 12\n",
        "Jo\u00e3o Braz de Aviz 12\n",
        "Joachim Meisner 12\n",
        "Rub\u00e9n Salazar G\u00f3mez 11\n",
        "Angelo Bagnasco 11\n",
        "Gabriel Zubeir Wako 11\n",
        "Dominik Duka 11\n",
        "Malcolm Ranjith 11\n",
        "John Tong Hon 11\n",
        "Godfried Danneels 11\n",
        "Laurent Monsengwo Pasinya 10\n",
        "Llu\u00eds Mart\u00ednez Sistach 10\n",
        "Tarcisio Bertone 10\n",
        "Jean-Claude Turcotte 10\n",
        "Juan Luis Cipriani Thorne 10\n",
        "Julio Terrazas Sandoval 10\n",
        "Kurt Koch 9\n",
        "Jaime Lucas Ortega y Alamino 9\n",
        "Giovanni Lajolo 9\n",
        "Anthony Olubunmi Okogie 9\n",
        "Daniel DiNardo 9\n",
        "Justin Francis Rigali 9\n",
        "Zenon Grocholewski 9\n",
        "Antonio Maria Vegli\u00f2 8\n",
        "Rainer Woelki 8\n",
        "Robert Sarah 8\n",
        "Gianfranco Ravasi 8\n",
        "Raymundo Damasceno Assis 8\n",
        "Fernando Filoni 7\n",
        "Attilio Nicora 7\n",
        "B\u00e9chara Boutros Ra\u00ef 7\n",
        "Antonio Ca\u00f1izares Llovera 7\n",
        "Jorge Bergoglio 7\n",
        "Carlo Caffarra 7\n",
        "Domenico Calcagno 7\n",
        "Leonardo Sandri 6\n",
        "Antonio Mar\u00eda Rouco Varela 6\n",
        "Jorge Urosa 6\n",
        "Karl Lehmann 6\n",
        "Crescenzio Sepe 6\n",
        "Telesphore Placidus Toppo 6\n",
        "Velasio de Paolis 6\n",
        "Francesco Monterisi 6\n",
        "Giuseppe Versaldi 6\n",
        "Giuseppe Bertello 6\n",
        "Stanislaw Rylko 6\n",
        "Reinhard Marx 6\n",
        "Francesco Coccopalmerio 6\n",
        "Francisco Javier Err\u00e1zuriz Ossa 6\n",
        "Santos Abril y Castell\u00f3 6\n",
        "Wilfrid Napier 6\n",
        "Nicol\u00e1s de Jes\u00fas L\u00f3pez Rodr\u00edguez 6\n",
        "Angelo Amato 6\n",
        "Antonios Naguib 6\n",
        "Manuel Monteiro de Castro 6\n",
        "Geraldo Majella Agnelo 6\n",
        "Severino Poletto 5\n",
        "Cl\u00e1udio Hummes 5\n",
        "Paolo Romeo 5\n",
        "Jean-Louis Tauran 5\n",
        "Paolo Sardi 5\n",
        "Juan Sandoval \u00cd\u00f1iguez 5\n",
        "Oswald Gracias 5\n",
        "Th\u00e9odore-Adrien Sarr 5\n",
        "Kazimierz Nycz 5\n",
        "Audrys Ba\u010dkis 5\n",
        "\u00d3scar Andr\u00e9s Rodr\u00edguez Maradiaga 5\n",
        "P\u00e9ter Erd\u0151 5\n",
        "Carlos Amigo Vallejo 5\n",
        "James Michael Harvey 5\n",
        "Polycarp Pengo 5\n",
        "Vinko Pulji\u0107 5\n",
        "Ivan Dias 5\n",
        "Josip Bozani\u0107 4\n",
        "Franc Rod\u00e9 4\n",
        "Andr\u00e9 Vingt-Trois 4\n",
        "Raffaele Farina 4\n",
        "John Njue 4\n",
        "Francisco Robles Ortega 4\n",
        "Angelo Comastri 4\n",
        "Norberto Rivera Carrera 3\n",
        "Mauro Piacenza 3\n",
        "Wim Eijk 3\n",
        "Ennio Antonelli 2\n",
        "Jean-Pierre Ricard 2\n",
        "Paul Josef Cordes 2\n",
        "Philippe Barbarin 1\n",
        "Giuseppe Betori 1\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course, the results will vary from execution to execution. Until the white smoke comes up, we can't really be sure how well this model is doing, but we can compare the rankings to those put together by experts and bookmakers, for example [this Business Insider story](http://www.businessinsider.com/who-will-replace-pope-benedict-2013-2?op=1). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conclusions, or:\n",
      "### Habemus papam?\n",
      "\n",
      "\n",
      "So how are we doing? Again, not great, but probably better than random. Using Wikipedia really seems to be biasing the model towards candidates from English-speaking countries, so we should probably be correcting for that -- if not in the model itself (maybe by dampening for Wikipedia entry length?) then in our analysis of the results. We could also consider using better sources for text data, for example the information available at http://www.ewtn.com/holysee/interregnum/electors.asp\n",
      "\n",
      "Also, you might have noticed that the model seems to consistently produce decisions in 2-3 rounds of voting -- almost certainly faster than real life. Think of how our elector agents are deciding their votes, by multiplying a similarity score between 0 and 1 by a vote count, reported in integers. If differences in preferences between two candidates are small, their effect will be swamped by even a single additional vote. We could consider better weighting rules that would produce better scaling, or add some more randomness to voting rounds besides the first.  \n",
      "\n",
      "Despite these issues, this model seems to be better than random. That's the power of computational social science -- we can get at least some insight by analyzing data that's readily available, and from models applying even very simple rules. A model like this doesn't need to be perfect to be useful; it can be another source of information, alongside historic results, expert opinion, and other forms of modeling.\n",
      "\n",
      "From here, you can keep on adding data and behaviors and build towards results that are as realistic (or at least believable) and robust as you can make them. Thanks for reading, and happy forecasting!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}