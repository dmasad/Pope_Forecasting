{
 "metadata": {
  "name": "Conclave_Modeling_Tutorial"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How (not) to Forecast the Papal Conclave\n",
      "========================================\n",
      "\n",
      "**TL;DR:** *This is a simple introduction to a few computational social science topics. It goes over some web scraping, processing the resulting text into a distance matrix, and using it as input into a simple agent-based model.*\n",
      "\n",
      "Forecasting a papal conclave seems like a fun challenge. A relatively small number of cardinal-electors under the age of 80 (115 of them, this time around), get together in the Sistine Chapel and vote four times a day until enough of them agree. Unlike with most elections, there aren't any voter polls, and there's no good historic data: cardinals take a vow of secrecy, so the only thing we know for sure is who is elected at the end of the process. Doctrine holds that the electors are guided by the Holy Spirit, but theology, regional politics, institutional rivalries and personal relationships undoubtedly play a role as well. \n",
      "\n",
      "So without good statistical data, how can we get our Nate Silver on? One way is an agent-based model (ABM). If we can make a reasonable guess as to how cardinals decides who to vote for we can simulate the conclave, running rounds of virtual votes until a Pope is elected.\n",
      "\n",
      "If we were serious about this, our next step would be to go and talk to Vatican experts to figure out how cardinals might vote. We would try and figure out who shared similar opinions on key issues, how the institutional politics and geographic considerations might play out. We'd look at age, and date of elevation, and we'd do our best to build up a social network -- who has worked with who before? Who are friends and allies, and who are the major rivalries? All of that matters.\n",
      "\n",
      "Remember the title? This is how **not** to forecast the conclave. We aren't going to do all of that. Instead, we'll just wing it. \n",
      "\n",
      "I'm going to propose two simple rules of thumb for how cardinals vote:\n",
      "\n",
      "1. Cardinals will tend to prefer candidates who are similar to themselves.\n",
      "2. As voting progresses, cardinals will vote for a candidate who is similar to themselves and also appears to have a chance of winning.\n",
      "\n",
      "(I'm also going to assume that the cardinals will elect one of their own, since that's been the case since 1378).\n",
      "\n",
      "These rules seem like a good starting point. They're wildly simplistic, but that's not necessarily a bad thing. Zero-intelligence agents have a long(ish) pedigree in ABM, and can be surprisingly successful at generating more complex phenomena. If it looks like we're on the right track, we can try to add additional rules, and see how much they improve the result."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Acquisition, or:\n",
      "### Looking for the car keys underneath the streetlight\n",
      "\n",
      "Now that we have our two rules of thumb, we need a way of figuring out just how similar cardinals are to one another. Again, if we wanted to do this properly, we would consult experts (if we aren't experts ourselves), look at high-quality data, read previous theory, and do all the other things that make up good science. Instead, we're going to do what lazy students have done since the beginning of time (about 2001 or so) -- we're going to consult Wikipedia.\n",
      "\n",
      "This is going to involve two separate steps: first, we need to get the data we want from Wikipedia (web scraping). Next, we need to use it to figure out how similar all our cardinal-electors are to one another."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Wikipedia Scraping\n",
      "\n",
      "This is a seat-of-the-pants project, but it doesn't mean we should just throw *all* good scientific practice out the window. Before we try and predict the next Pope, we should see whether we could have predicted the last one. So let's start by grabbing data on the 2005 conclave."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two important libraries for web scraping with Python: **urllib2**, which is part of the standard library and will read web pages for us, and **[Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/)** which will parse the web page  for us.\n",
      "\n",
      "We'll also use the standard **json** library to store the data we download.\n",
      "\n",
      "Let's start by importing them:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division # Makes division work the way we want it to\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "import json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we define some important web addresses so that we won't need to type them out later. We want the base Wikipedia URL, the URL of Wikipedia's [JSON API](http://www.mediawiki.org/wiki/API:Data_formats) and the actual page with the list of 2005's electors. Finally, we also define an HTML header, which will let Python mimic a web browser when it accesses Wikipedia."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BASE_URL = \"http://en.wikipedia.org\"\n",
      "FIRST_URL = \"http://en.wikipedia.org/wiki/Cardinal_electors_in_Papal_conclave,_2005\"\n",
      "WIKI_API_JSON = \"http://en.wikipedia.org/w/api.php?action=parse&format=json&redirects=true&page=\"\n",
      "\n",
      "header = {'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Web scraping is faster than copying and pasting hundreds of links by hand, but it still requires some work: we need to look at the HTML structure of the target page and find any regularities we can exploit to identify the data we want to extract. In this case, we want to extract the names of the electors, and links to their own Wikipedia pages. \n",
      "\n",
      "I've already done the work of that; look at the source of the page yourself if you want to see it for yourself."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First, we open the page with the data we want:\n",
      "req = urllib2.Request(FIRST_URL,None,header)\n",
      "page = urllib2.urlopen(req)\n",
      "page_text = page.read()\n",
      "\n",
      "# Next, we use Beautiful Soup to find the page elements we care about:\n",
      "soup = BeautifulSoup(page_text)\n",
      "\n",
      "# Get all the cardinal electors from the page\n",
      "ol = soup.findAll(\"ol\")\n",
      "links = []\n",
      "for lst in ol:\n",
      "    items = lst.findAll(\"li\")\n",
      "    for i in items:\n",
      "        links.append(i)\n",
      "\n",
      "# Check how many links we got; should be 115\n",
      "print len(links)\n",
      "\n",
      "# Now extract the name and URL for each elector:\n",
      "# Extract the names and links:\n",
      "electors = []\n",
      "for entry in links:\n",
      "    first_link = entry.find(\"a\")\n",
      "    name = first_link.get_text()\n",
      "    url = first_link.get(\"href\")\n",
      "    electors.append({\"Name\": name, \"url\": url})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "115\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the list of cardinals, we need to get all of their pages. With the list of electors, we wanted only a specific subset of the page; now, we just want the entire page text, so we can use Wikipedia's JSON API."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the text for each via the Wikimedia JSON API:\n",
      "for elector in electors:\n",
      "    url = elector[\"url\"][6:] # Drop the '/wiki/'\n",
      "    page = urllib2.urlopen(WIKI_API_JSON + url).read()\n",
      "    page_json = json.loads(page)\n",
      "    page_text = page_json[\"parse\"][\"text\"].values()[0]\n",
      "    page_soup = BeautifulSoup(page_text)\n",
      "    text = page_soup.get_text()\n",
      "    elector[\"FullText\"] = text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Look through the electors dictionaries -- you'll see that for each cardinal, we now have the full text of their Wikipedia page. \n",
      "\n",
      "We should save this data to keep us from needing to rescrape the data from scratch every time we want to run the model. The easiest way is to simply dump our dictionaries to a JSON file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"ElectorData05.json\", \"wb\") as f:\n",
      "    json.dump(electors, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the text of each cardinal's Wikipedia page, we need to use it to find some similarity measure between them. \n",
      "\n",
      "Specifically: let's the define the similarity between two cardinals as the similarity between their English Wikipedia pages. This is almost certainly a highly flawed measure, but hey, it's easy.\n",
      "\n",
      "More formally, given two Wikipedia pages (or any other text documents), we define the similarity between them as follows:\n",
      "\n",
      "1. Separate each document into words, removing [stop-words](http://en.wikipedia.org/wiki/Stop_words) and punctuation.\n",
      "2. Group the words into n-grams. An n-gram is simply a sequence of n words that come after each other. For example, in *\"The quick brown fox jumped over the lazy dog\"*, unigrams (n=1) are just the words; bigrams (n=2) are (The quick), (quick brown), (brown fox), etc. \n",
      "3. Use these n-grams to create [Term Vectors](http://en.wikipedia.org/wiki/Term_vector_model), which just count the number of times each n-gram appears in the document. \n",
      "4. Finally, compute the [Jaccard Similarity](http://en.wikipedia.org/wiki/Jaccard_index) between the documents. This simply means counting the n-grams that appear in both documents, and dividing that by the total number of n-grams that appear in either document. The higher the fraction of n-grams that appear in both documents, the more similar they are.\n",
      "\n",
      "Let's define some functions that will help us do this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_word(word, punctuation):\n",
      "    \"\"\"\n",
      "    Cleans all the punctuation marks from a word.\n",
      "\n",
      "    Args:\n",
      "        word: target word\n",
      "        punctuation: string of punctuation tokens.\n",
      "    Returns:\n",
      "        The word cleaned of punctuation marks at the beginning and end, and\n",
      "        converted to lower-case.\n",
      "    \"\"\"\n",
      "    try: \n",
      "        while len(word)>0:\n",
      "            while word[0] in punctuation:\n",
      "                word = word[1:]\n",
      "            while word[-1] in punctuation:\n",
      "                word = word[:-1]\n",
      "            break\n",
      "        return word.lower()\n",
      "    \n",
      "    except:\n",
      "        return \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_ngrams(text, n=1, punctuation=\"\"\",\"';:-?!.\"\"\", stop_words=[]):\n",
      "    \"\"\"\n",
      "    Sparse-vectorizer: cleans the text and returns a dictionary of n-gram counts\n",
      "\n",
      "    Args:\n",
      "        text: the input text; assumed to be a sentence.\n",
      "        n: the number of tokens per n-gram. Defaults to 1.\n",
      "        punctuation: String of punctuation tokens to ignore.\n",
      "        stop_words: a list of n-grams to ignore; empty by default.\n",
      "    \n",
      "    Returns:\n",
      "        A term vector, of the form\n",
      "            {term: count, term: count...}\n",
      "    \"\"\"\n",
      "    ngrams = defaultdict(int)\n",
      "\n",
      "    words = [clean_word(word, punctuation) for word in text.split()] # Clean the word list\n",
      "    words = [word for word in words if len(word)>0] #Remove nulls.\n",
      "    while n > 0:\n",
      "        # Repeat for all values >= n:        \n",
      "        for i in range(len(words)-n+1):\n",
      "            new_ngram = []\n",
      "            counter = 0\n",
      "            while len(new_ngram)<n:\n",
      "                new_ngram.append(words[i+counter])\n",
      "                counter += 1\n",
      "            new_ngram = tuple(new_ngram)\n",
      "            if new_ngram not in stop_words:\n",
      "                ngrams[new_ngram] += 1\n",
      "        n -= 1\n",
      "    \n",
      "    return ngrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jaccard_similarity(vector1, vector2):\n",
      "    '''\n",
      "    Compute Jaccard Similarity between two sparse vectors,\n",
      "    represented as dicts.\n",
      "    '''\n",
      "\n",
      "    all_keys = set(vector1.keys() + vector2.keys())\n",
      "    union = len(all_keys)\n",
      "    intersection = 0\n",
      "    for key in all_keys:\n",
      "        if key in vector1 and key in vector2:\n",
      "            intersection += 1\n",
      "    return intersection / union"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_distance_matrix(sparse_vectors, metric, inverse=False):\n",
      "    '''\n",
      "    Inputs are sparse vectors and a metric function; \n",
      "    pre-computes all the distances and returns a function \n",
      "    for accessing them. \n",
      "\n",
      "    Args:\n",
      "        sparse_vectors: dictionary of {'name': {sparse: vectors},...}\n",
      "        metric: distance metric function\n",
      "        inverse: bool on whether to invert the metric.\n",
      "                If False, smaller is more similar.\n",
      "\n",
      "    Returns:\n",
      "        A closure-defined function wrapped around a pre-computed Similarity\n",
      "        matrix.\n",
      "        \n",
      "    '''\n",
      "    distances = {}\n",
      "    for doc1, vector1 in sparse_vectors.items():\n",
      "        distances[doc1] = {}\n",
      "        for doc2, vector2 in sparse_vectors.items():\n",
      "            if doc1 != doc2 and doc2 not in distances:\n",
      "                distances[doc1][doc2] = metric(vector1, vector2)\n",
      "                if inverse: \n",
      "                    distances[doc1][doc2] = 1 - distances[doc1][doc2]\n",
      "\n",
      "    def find_distance(doc1, doc2):\n",
      "        '''\n",
      "        Find the distance between doc1 and doc2\n",
      "        '''\n",
      "        if doc2 not in distances[doc1]:\n",
      "            doc1, doc2 = doc2, doc1\n",
      "        return distances[doc1][doc2]\n",
      "    return find_distance"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great! Now, how do we use our newly-defined functions? Let's actually apply them to our data. If you have the excellent [NLTK](http://nltk.org/) library installed, you can use its list of stopwords; otherwise, I'm copying it here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define stop-words:\n",
      "try:\n",
      "    from nltk.corpus import stopwords\n",
      "    all_stopwords = stopwords.words(\"english\") + [\"[edit]\"] \n",
      "except:\n",
      "    all_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
      "        'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
      "         'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', \n",
      "         'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
      "         'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', \n",
      "         'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
      "         'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',\n",
      "         'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', \n",
      "         'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n",
      "         'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
      "         'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', \n",
      "         'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
      "         'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', '[edit]']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate the term vector for each page:\n",
      "for elector in electors:\n",
      "    elector[\"TextVector\"] = find_ngrams(elector[\"FullText\"], n=3, stop_words = all_stopwords)\n",
      "\n",
      "vectors = {} # Dictionary to hold our vectors\n",
      "# Generate the distance matrix, and the function that accesses it:\n",
      "for elector in electors:\n",
      "    vectors[elector[\"Name\"]] = elector[\"TextVector\"]\n",
      "find_distance = create_distance_matrix(vectors, jaccard_similarity, inverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see how we're doing, let's generate a simple data quality test and see which cardinal is most similar (by this measure) to the rest of the cardinals."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean_similarities = {}\n",
      "# Iterate over each elector, and compute his similarity to each other one.\n",
      "for elector1 in electors:\n",
      "    total_similarities = 0\n",
      "    for elector2 in electors:\n",
      "        if elector1 == elector2: continue # Don't compare someone to themselves\n",
      "        similarity = find_distance(elector1[\"Name\"], elector2[\"Name\"])\n",
      "        total_similarities += similarity\n",
      "    mean_similarity = total_similarities / (len(electors)-1)\n",
      "    mean_similarities[elector1[\"Name\"]] = mean_similarity\n",
      "\n",
      "# Print the electors, sorted from highest to lowest mean similarity:\n",
      "print sorted(mean_similarities.keys(), key=lambda x: mean_similarities[x], reverse=True)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Joseph Ratzinger', u'George Pell', u'Jean-Marie Lustiger', u\"Keith O'Brien\", u'Carlo Maria Martini', u'Francis George', u'Tarcisio Bertone', u'Angelo Scola', u'Roger Mahony', u\"Cormac Murphy-O'Connor\", u'Edward Egan', u'Bernard Francis Law', u'Miguel Obando y Bravo', u'Peter Turkson', u'Justin Rigali', u'Angelo Sodano', u'J\\xf3zef Glemp', u'Dar\\xedo Castrill\\xf3n Hoyos', u'Marc Ouellet', u'Christoph Sch\\xf6nborn', u'Dionigi Tettamanzi', u'Desmond Connell', u'Francis Arinze', u'Lubomyr Husar', u'Adam Maida', u'Walter Kasper', u'Fr\\xe9d\\xe9ric Etsou-Nzabi-Bamungwabi', u'Renato Raffaele Martino', u'Miloslav Vlk', u'Jean-Claude Turcotte', u'Thomas Stafford Williams', u'Godfried Danneels', u'Giovanni Battista Re', u'Jorge Bergoglio', u'Theodore McCarrick', u'Jos\\xe9 da Cruz Policarpo', u'Eus\\xe9bio Scheid', u'James Francis Stafford', u'Mar Varkey Vithayathil', u'Juan Luis Cipriani Thorne', u'Ricardo Vidal', u'Camillo Ruini', u'Ivan Dias', u'Alfonso L\\xf3pez Trujillo', u'Michele Giordano', u'Joachim Meisner', u'Jean-Louis Tauran', u'Edmund Casimir Szoka', u'Jaime Lucas Ortega y Alamino', u'Juli\\xe1n Herranz Casado', u'Giacomo Biffi', u'William Keeler', u'\\xd3scar Andr\\xe9s Rodr\\xedguez Maradiaga', u'Ignace Daoud', u'William Wakefield Baum', u'Karl Lehmann', u'Norberto Rivera Carrera', u'Jorge Arturo Medina Est\\xe9vez', u'Adrianus Johannes Simonis', u'Jos\\xe9 Freire Falc\\xe3o', u'Francisco Javier Err\\xe1zuriz Ossa', u'Javier Lozano Barrag\\xe1n', u'L\\xe1szl\\xf3 Paskai', u'Michael Kitbunchu', u'Georg Sterzinsky', u'Crescenzio Sepe', u'Zenon Grocholewski', u'Aloysius Ambrozic', u'Juan Sandoval \\xcd\\xf1iguez', u'Francesco Marchisano', u'Mario Francesco Pompedda', u'Cl\\xe1udio Hummes', u'Vinko Pulji\\u0107', u'Emmanuel Wamala', u'J\\u0101nis Puj\\u0101ts', u'Julius Riyadi Darmaatmadja', u'Ennio Antonelli', u'Sergio Sebastiani', u'Armand Razafindratandra', u'Attilio Nicora', u'Agostino Cacciavillan', u'Jean-Baptiste Ph\\u1ea1m Minh M\\u1eabn', u'Telesphore Toppo', u'Polycarp Pengo', u'Antonio Mar\\xeda Rouco Varela', u'Stephen Fumio Hamao', u'P\\xe9ter Erd\\u0151', u'Paul Poupard', u'Eduardo Mart\\xednez Somalo', u'Pedro Rubiano S\\xe1enz', u'Rodolfo Quezada Toru\\xf1o', u'Carlos Amigo Vallejo', u'Wilfrid Fox Napier', u'Friedrich Wetter', u'Anthony Olubumni Okogie', u'Francisco \\xc1lvarez Mart\\xednez', u'Nicol\\xe1s de Jes\\xfas L\\xf3pez Rodr\\xedguez', u'Henri Schwery', u'Marian Jaworski', u'Geraldo Majella Agnelo', u'Peter Shirayanagi', u'Franciszek Macharski', u'Salvatore De Giorgi', u'Christian Tumi', u'Julio Terrazas Sandoval', u'Philippe Barbarin', u'Josip Bozani\\u0107', u'Audrys Juozas Ba\\u010dkis', u'Ricardo Mar\\xeda Carles Gord\\xf3', u'Bernard Agr\\xe9', u'Bernard Panafieu', u'Gabriel Zubeir Wako', u'Marco C\\xe9', u'Jos\\xe9 Saraiva Martins', u'Severino Poletto']\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Look at the first name on the list. Well, what do you know?\n",
      "\n",
      "Of course, you *do* know. Since Ratzinger was elected Pope, his Wikipedia page is probably longer and more detailed than anyone else's, so of course there will be more similarities. [According to Wikipedia](http://en.wikipedia.org/wiki/Papal_conclave,_2005#Results_of_the_first_ballot), Carlo Maria Martini (ranked 5th on the list above) and Camillo Ruini (ranked far lower) were the other two serious contenders, while Jean-Marie Lustiger (ranked 3rd) doesn't even appear on the [list of potential candidates](http://en.wikipedia.org/wiki/List_of_papabili_in_the_2005_papal_conclave). So, far from perfect. But not terrible either.\n",
      "\n",
      "### Next: Zero-Intelligence Cardinals!\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}